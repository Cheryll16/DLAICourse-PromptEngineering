{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Summarizing**\n",
        "\n",
        "In this session, we will learn how to use ChatGPT to summarize text with a focus on specific topics."
      ],
      "metadata": {
        "id": "7P2JpubHMSiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set Up**"
      ],
      "metadata": {
        "id": "XkDOPyaBSzKC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SY64ZW2MNcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4133ef34-d0a7-4242-84e3-47dbbd8b0e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m71.7/73.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "import os\n",
        "\n",
        "\"\"\"\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
        "\"\"\"\n",
        "openai.api_key = \"sk-XXX\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"): # Andrew mentioned that the prompt/ completion paradigm is preferable for this class\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n"
      ],
      "metadata": {
        "id": "LhrWnPXEMYqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text to summarize**"
      ],
      "metadata": {
        "id": "um5JL0OrMcOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prod_review = \"\"\"\n",
        "Got this panda plush toy for my daughter's birthday, \\\n",
        "who loves it and takes it everywhere. It's soft and \\\n",
        "super cute, and its face has a friendly look. It's \\\n",
        "a bit small for what I paid though. I think there \\\n",
        "might be other options that are bigger for the \\\n",
        "same price. It arrived a day earlier than expected, \\\n",
        "so I got to play with it myself before I gave it \\\n",
        "to her.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IwgERs7PMdBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summarize with a word/sentence/character limit**"
      ],
      "metadata": {
        "id": "r86gnNmKMjJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, I tried different prompt to keep the answer short. The first one is the code given by the course instructor, which still returns more than 30 words for me. So I tried some variant expressions to force ChatGPT to give an answer within a limit."
      ],
      "metadata": {
        "id": "Tiet60ivUfk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DLAI Course example: ...., in at most 30 words\n",
        "prompt = f\"\"\"\n",
        "Your task is to generate a short summary of a product \\\n",
        "review from an ecommerce site.\n",
        "\n",
        "Summarize the review below, delimited by triple\n",
        "backticks, in at most 30 words.\n",
        "\n",
        "Review: ```{prod_review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "a9SXEe6HMliI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917991e2-bfbd-402f-c235-5558ce38cad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This review praises the panda plush toy for being soft, cute, and having a friendly face. However, the reviewer feels it is small for the price and suggests there may be larger options available. The toy arrived earlier than expected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.count(\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeG4U5_vTXa1",
        "outputId": "2ab488d1-5507-44f4-c943-377304bf2683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Variant 1: ..., in no more than 30 words\n",
        "prompt = f\"\"\"\n",
        "Your task is to generate a short summary of a product \\\n",
        "review from an ecommerce site.\n",
        "\n",
        "Summarize the review below, delimited by triple\n",
        "backticks, in no more than 30 words.\n",
        "\n",
        "Review: ```{prod_review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYzQb7q-Tw8P",
        "outputId": "0010919b-b80e-46ca-c1b0-046d8ecd80de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This review praises the panda plush toy for being soft, cute, and having a friendly face. However, the reviewer feels it is small for the price and suggests there may be larger options available. The toy arrived earlier than expected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.count(\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5tukjykT3jW",
        "outputId": "33eac0a6-f63c-443b-b5c8-7e3889edecfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Variant 2: ..., in less than 30 words\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to generate a short summary of a product \\\n",
        "review from an ecommerce site.\n",
        "\n",
        "Summarize the review below, delimited by triple\n",
        "backticks, in less than 30 words.\n",
        "\n",
        "Review: ```{prod_review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi2rmYG_UB96",
        "outputId": "9ba056c6-7430-4b58-f396-194e9e884220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This review praises the panda plush toy for being soft, cute, and having a friendly face. However, the reviewer feels it is small for the price and suggests there may be larger options available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.count(\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UvydnUCT_o4",
        "outputId": "4993d7af-3160-4ecc-b2f2-d53bea816c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variant 4: Change the word limit instruction into a single imperative sentence.\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to generate a short summary of a product \\\n",
        "review from an ecommerce site.\n",
        "\n",
        "Summarize the review below, delimited by triple\n",
        "backticks.\n",
        "\n",
        "You must keep your answer with no more than 30 words.\n",
        "\n",
        "Review: ```{prod_review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al8hVw0QURCH",
        "outputId": "34bd1620-444f-4992-c14f-87473f275060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soft and cute panda plush toy loved by daughter, but a bit small for the price. Arrived early.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.count(\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeAxFSk_Ua6l",
        "outputId": "77bdb02a-8210-4941-b78a-3f3190a767f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Personal Opinion:**\n",
        "\n",
        "I tried different variant expressions to indicate ChatGPT to give me an answer within 30 words, and I personally find imperative sentence with negative words work better. Also, if you need ChatGPT to strongly satisfy your instructions, you would better write your imperative indication **SEPERATELY** in a single sentence with **NEGATIVE** expressive style (e.g. \"mo more\" better than \"less\")."
      ],
      "metadata": {
        "id": "7_pwkylWVftH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summarize with a focus on shipping and delivery**\n",
        "\n",
        "Here is to show how to summarize a text for specific information."
      ],
      "metadata": {
        "id": "tKvQw2V4MxrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to generate a short summary of a product \\\n",
        "review from an ecommerce site to give feedback to the \\\n",
        "Shipping deparmtment.\n",
        "\n",
        "Summarize the review below, delimited by triple\n",
        "backticks, in at most 30 words, and focusing on any aspects \\\n",
        "that mention shipping and delivery of the product.\n",
        "\n",
        "Review: ```{prod_review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "qTW05YCwMzG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0144e3b5-eeb5-43bd-90de-e5e1eeb186f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The customer is happy with the product but mentions that it is smaller than expected. They also mention that the shipping was faster than expected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summarize with a focus on price and value**"
      ],
      "metadata": {
        "id": "E8EVO9XdM_jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to generate a short summary of a product \\\n",
        "review from an ecommerce site to give feedback to the \\\n",
        "pricing deparmtment, responsible for determining the \\\n",
        "price of the product.\n",
        "\n",
        "Summarize the review below, delimited by triple\n",
        "backticks, in at most 30 words, and focusing on any aspects \\\n",
        "that are relevant to the price and perceived value.\n",
        "\n",
        "Review: ```{prod_review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "CSxwSbhCNAi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db280545-def3-48cd-baf2-f07838dd89f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reviewer loves the panda plush toy for its softness and cuteness, but feels it is overpriced compared to other options.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment\n",
        "Summaries include topics that are not related to the topic of focus."
      ],
      "metadata": {
        "id": "VcoJg41tNFdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Try \"extract\" instead of \"summarize\"**"
      ],
      "metadata": {
        "id": "BcgYwahzNKb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to extract relevant information from \\\n",
        "a product review from an ecommerce site to give \\\n",
        "feedback to the Shipping department.\n",
        "\n",
        "From the review below, delimited by triple quotes \\\n",
        "extract the information relevant to shipping and \\\n",
        "delivery. Limit to 30 words.\n",
        "\n",
        "Review: ```{prod_review}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "sobx8s2pNMOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcf6a7a6-05c5-4d37-e58c-5e7fcac055b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The relevant information for the Shipping department is that the product arrived a day earlier than expected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summarize multiple product reviews**"
      ],
      "metadata": {
        "id": "KXHxx33hOSiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_1 = prod_review #review_1 is missing on purpose\n",
        "\n",
        "# review for a standing lamp\n",
        "review_2 = \"\"\"\n",
        "Needed a nice lamp for my bedroom, and this one \\\n",
        "had additional storage and not too high of a price \\\n",
        "point. Got it fast - arrived in 2 days. The string \\\n",
        "to the lamp broke during the transit and the company \\\n",
        "happily sent over a new one. Came within a few days \\\n",
        "as well. It was easy to put together. Then I had a \\\n",
        "missing part, so I contacted their support and they \\\n",
        "very quickly got me the missing piece! Seems to me \\\n",
        "to be a great company that cares about their customers \\\n",
        "and products.\n",
        "\"\"\"\n",
        "\n",
        "# review for an electric toothbrush\n",
        "review_3 = \"\"\"\n",
        "My dental hygienist recommended an electric toothbrush, \\\n",
        "which is why I got this. The battery life seems to be \\\n",
        "pretty impressive so far. After initial charging and \\\n",
        "leaving the charger plugged in for the first week to \\\n",
        "condition the battery, I've unplugged the charger and \\\n",
        "been using it for twice daily brushing for the last \\\n",
        "3 weeks all on the same charge. But the toothbrush head \\\n",
        "is too small. I’ve seen baby toothbrushes bigger than \\\n",
        "this one. I wish the head was bigger with different \\\n",
        "length bristles to get between teeth better because \\\n",
        "this one doesn’t.  Overall if you can get this one \\\n",
        "around the $50 mark, it's a good deal. The manufactuer's \\\n",
        "replacements heads are pretty expensive, but you can \\\n",
        "get generic ones that're more reasonably priced. This \\\n",
        "toothbrush makes me feel like I've been to the dentist \\\n",
        "every day. My teeth feel sparkly clean!\n",
        "\"\"\"\n",
        "\n",
        "# review for a blender\n",
        "review_4 = \"\"\"\n",
        "So, they still had the 17 piece system on seasonal \\\n",
        "sale for around $49 in the month of November, about \\\n",
        "half off, but for some reason (call it price gouging) \\\n",
        "around the second week of December the prices all went \\\n",
        "up to about anywhere from between $70-$89 for the same \\\n",
        "system. And the 11 piece system went up around $10 or \\\n",
        "so in price also from the earlier sale price of $29. \\\n",
        "So it looks okay, but if you look at the base, the part \\\n",
        "where the blade locks into place doesn’t look as good \\\n",
        "as in previous editions from a few years ago, but I \\\n",
        "plan to be very gentle with it (example, I crush \\\n",
        "very hard items like beans, ice, rice, etc. in the \\\n",
        "blender first then pulverize them in the serving size \\\n",
        "I want in the blender then switch to the whipping \\\n",
        "blade for a finer flour, and use the cross cutting blade \\\n",
        "first when making smoothies, then use the flat blade \\\n",
        "if I need them finer/less pulpy). Special tip when making \\\n",
        "smoothies, finely cut and freeze the fruits and \\\n",
        "vegetables (if using spinach-lightly stew soften the \\\n",
        "spinach then freeze until ready for use-and if making \\\n",
        "sorbet, use a small to medium sized food processor) \\\n",
        "that you plan to use that way you can avoid adding so \\\n",
        "much ice if at all-when making your smoothie. \\\n",
        "After about a year, the motor was making a funny noise. \\\n",
        "I called customer service but the warranty expired \\\n",
        "already, so I had to buy another one. FYI: The overall \\\n",
        "quality has gone done in these types of products, so \\\n",
        "they are kind of counting on brand recognition and \\\n",
        "consumer loyalty to maintain sales. Got it in about \\\n",
        "two days.\n",
        "\"\"\"\n",
        "\n",
        "reviews = [review_1, review_2, review_3, review_4]\n",
        "\n"
      ],
      "metadata": {
        "id": "03baP037ObP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The code is provided by the instructor. It is normal if you got RateLimitError on requests per minute if you only have a free account.\n",
        "#But no worry I will give you a trouble shooter afterwards.\n",
        "\n",
        "for i in range(len(reviews)):\n",
        "    prompt = f\"\"\"\n",
        "    Your task is to generate a short summary of a product \\\n",
        "    review from an ecommerce site.\n",
        "\n",
        "    Summarize the review below, delimited by triple \\\n",
        "    backticks in at most 20 words.\n",
        "\n",
        "    Review: ```{reviews[i]}```\n",
        "    \"\"\"\n",
        "\n",
        "    response = get_completion(prompt)\n",
        "    print(i, response, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "68PsJi_6iPcI",
        "outputId": "267d0004-4a0d-41f8-d07f-94abae757b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Soft and cute panda plush toy loved by daughter, but small for the price. Arrived early. \n",
            "\n",
            "1 Great lamp with storage, fast delivery, excellent customer service, and easy assembly. Highly recommended. \n",
            "\n",
            "2 The reviewer recommends the electric toothbrush for its impressive battery life, but criticizes the small brush head. \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-cd8f04853d00>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b02ce4bdae67>\u001b[0m in \u001b[0;36mget_completion\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Andrew mentioned that the prompt/ completion paradigm is preferable for this class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-NMdO1tMTi0F8FxpUGHYxFdgJ on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trouble Shooter for RateLimitError on Requests per min.:**\n",
        "\n",
        "We use the tenacity.retry decorator to add exponential backoff to requests.\n",
        "\n",
        "I modified the solution provided by [official openai cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb) and you can also find other variant solutions on the page."
      ],
      "metadata": {
        "id": "i753H_3ejqpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use library tenacity to rewrite the get_completion function into get_completion_withbackoff function.\n",
        "\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")  # for exponential backoff\n",
        "\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=30, max=60), stop=stop_after_attempt(30))\n",
        "def get_completion_withbackoff(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "vBXb4cUAgPA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(reviews)):\n",
        "    prompt = f\"\"\"\n",
        "    Your task is to generate a short summary of a product \\\n",
        "    review from an ecommerce site.\n",
        "\n",
        "    Summarize the review below, delimited by triple \\\n",
        "    backticks in at most 20 words.\n",
        "\n",
        "\n",
        "    Review: ```{reviews[i]}```\n",
        "    \"\"\"\n",
        "\n",
        "    response = get_completion_withbackoff(prompt)\n",
        "    print(i, response, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FucmcFmSbI4N",
        "outputId": "8ec5a01d-66b3-433f-dcc6-7c0a0dc512cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Soft and cute panda plush toy loved by daughter, but small for the price. Arrived early. \n",
            "\n",
            "1 Great lamp with storage, fast delivery, excellent customer service, and easy assembly. Highly recommended. \n",
            "\n",
            "2 Battery life is impressive, but toothbrush head is too small. Good deal if bought around $50. \n",
            "\n",
            "3 The reviewer found the price increase after the sale disappointing and noticed a decrease in quality. \n",
            "\n"
          ]
        }
      ]
    }
  ]
}